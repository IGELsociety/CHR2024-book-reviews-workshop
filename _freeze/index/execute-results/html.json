{
  "hash": "f3451773465626df98e7f5bcb85b619e",
  "result": {
    "markdown": "---\ntitle: \"Larger-Than-Memory Data Workflows with Apache Arrow\"\n---\n\n\n## Workshop Description\n\nAs datasets become larger and more complex, the boundaries between data engineering and data science are becoming blurred. Data analysis pipelines with larger-than-memory data are becoming commonplace, creating a gap that needs to be bridged: between engineering tools designed to work with very large datasets on the one hand, and data science tools that provide the analysis capabilities used in data workflows on the other. One way to build this bridge is with [Apache Arrow](https://arrow.apache.org/), a multi-language toolbox for working with larger-than-memory tabular data. Arrow is designed to improve performance and efficiency, and places emphasis on standardization and interoperability among workflow components, programming languages, and systems. The **arrow** package provides a mature R interface to Apache Arrow, making it an appealing solution for data scientists working with large data in R.\n\n::: {.column-margin}\n![](img/arrow-hex-dark.png)\n:::\n\nIn this tutorial you will learn how to use the **arrow** R package to create seamless engineering-to-analysis data pipelines. You’ll learn how to use interoperable data file formats like Parquet or Feather for efficient storage and data access. You’ll learn how to exercise fine control over data types to avoid common data pipeline problems. During the tutorial you’ll be processing larger-than-memory files and multi-file datasets with familiar **dplyr** syntax, and working with data in cloud storage. The tutorial doesn’t assume any previous experience with Apache Arrow: instead, it will provide a foundation for using **arrow**, giving you access to a powerful suite of tools for analyzing larger-than-memory datasets in R.\n\nGitHub Repository: [github.com/djnavarro/arrow-user2022](https://github.com/djnavarro/arrow-user2022)\n\n## Instructors \n\n- [Danielle Navarro](https://djnavarro.net/) - Danielle is a data scientist, professional educator, generative artist, former academic in recovery, open source R developer, and author of multiple books on statistics and data analysis. \n- [Jonathan Keane](https://jonkeane.com/) - Jonathan is an engineering and data science manager at Voltron Data. They've been passionate about R since undergrad and developed or contributed to a number of open source projects over the years.\n- [Stephanie Hazlitt](https://twitter.com/stephhazlitt) - Stephanie is a data scientist, an avid R user, and an engineering manager at Voltron Data, with a passion for supporting people and teams in learning, creating and sharing data science products and tools.\n\n## Tutorial Content\n\n- [0: Packages and Data](packages-and-data.html). Some instructions on the packages and data sets used in the workshop. It would be handy to read this before the workshop starts!\n- [1: Hello Arrow](hello-arrow.html). The first session of the workshop provides an overview of the Apache Arrow project and gives participants their first hands on experience working with data using Arrow. \n- [2: Data Wrangling](data-wrangling.html). The second session is a deep dive into the analyzing large data sets using **arrow**, **dplyr**, and to a lesser extent **duckdb**. This is the longest session of the workshop.\n- [3: Data Storage](data-storage.html). The third session looks in detail the read/write capabilities of **arrow**. It discusses the parquet file format, how to use it effectively for large data sets, and how to partition large data sets across many files.\n- [4: Advanced Arrow](advanced.html). The final session is brief, and takes a look under the hood. It talks about the data structures and data types used in Arrow.\n\n## Quick Start Guide\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# download a copy of this repository\nusethis::create_from_github(\n  repo_spec = \"djnavarro/arrow-user2022\", \n  destdir=\"<your chosen path>\"\n)\n\n# install the package dependencies\nremotes::install_deps()\n\n# manually download and unzip the \"tiny taxi\" data\ndownload.file(\n  url = \"https://github.com/djnavarro/arrow-user2022/releases/download/v0.1/nyc-taxi-tiny.zip\",\n  destfile = here::here(\"data/nyc-taxi-tiny.zip\")\n)\nunzip(\n  zipfile = here::here(\"data/nyc-taxi-tiny.zip\"), \n  exdir = here::here(\"data\")\n)\n```\n:::\n\n\n## When/Where\n\n\n::: {.cell}\n\n```{.r .cell-code}\nworkshop_time <- function(tz) {\n  start <- lubridate::ymd_hms(\"2022-06-20 14:00:00\", tz = \"America/Chicago\")\n  close <- lubridate::ymd_hms(\"2022-06-20 17:30:00\", tz = \"America/Chicago\")\n  cat(\"For time zone:\", tz, \"\\n\")\n  cat(\"  start time:\", lubridate::with_tz(start, tz) |> as.character(), \"\\n\")\n  cat(\"  close time:\", lubridate::with_tz(close, tz) |> as.character(), \"\\n\")\n}\n\nworkshop_time(\"America/Vancouver\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor time zone: America/Vancouver \n  start time: 2022-06-20 12:00:00 \n  close time: 2022-06-20 15:30:00 \n```\n:::\n\n```{.r .cell-code}\nworkshop_time(\"America/Chicago\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor time zone: America/Chicago \n  start time: 2022-06-20 14:00:00 \n  close time: 2022-06-20 17:30:00 \n```\n:::\n\n```{.r .cell-code}\nworkshop_time(\"Africa/Harare\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor time zone: Africa/Harare \n  start time: 2022-06-20 21:00:00 \n  close time: 2022-06-21 00:30:00 \n```\n:::\n\n```{.r .cell-code}\nworkshop_time(\"Australia/Sydney\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor time zone: Australia/Sydney \n  start time: 2022-06-21 05:00:00 \n  close time: 2022-06-21 08:30:00 \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}